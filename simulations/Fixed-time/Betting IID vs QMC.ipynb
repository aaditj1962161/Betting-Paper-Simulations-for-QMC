{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CI width by changing sample size for QMC and IID Beta for the Hedged (Betting) CI Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[This JRSSB article by Ian Waudby-Smith and Aaditya Ramdas](https://academic.oup.com/jrsssb/article/86/1/1/7043257) takes $X_1, X_2, \\ldots \\stackrel{\\text{IID}}{\\sim} F$ and computes a sequential confidence interval for $\\mu = \\mathbb{E}(X)$.\n",
    "\n",
    "For Quasi-Monte Carlo (QMC), also know as low discrepancy sequences, we are going to take \n",
    "\n",
    "$$\n",
    "X_i = \\frac{1}{n} \\sum_{j=1}^n T_{ij},\n",
    "$$ \n",
    "\n",
    "where for each $i$, $\\{T_{ij}\\}_{j=1}^n$ is a QMC set that mimics $F$. Therefore, $X_i$ is close to $\\mu$, and the sequence $\\{X_i\\}_{i=1}^R$ is an IID sequence based on $N = nR$ samples.\n",
    "\n",
    "In this notebook, $F$ is a Beta Distribution.\n",
    "\n",
    "\n",
    "Similarly, for QMC, for $Y = f(X)$ where $X \\sim U(0, 1)$ and $\\mu = \\mathbb{E}(Y) = \\mathbb{E}(f(X))$, we are going to take\n",
    "\n",
    "$$\n",
    "Y_i = \\frac{1}{n} \\sum_{j=1}^n f(x_{ij})\n",
    "$$ \n",
    "\n",
    "Therefore, $Y_i$ is close to $\\mu$, and the sequence $\\{Y_i\\}_{i=1}^R$ is an IID sequence based on $N = nR$ samples. \n",
    "\n",
    "In this notebook, we use two integrands: \n",
    "\n",
    "$Y = f(X) = \\frac{X e^X}{e}$\n",
    "\n",
    "$Y =\n",
    "f(X,Y) = \n",
    "\\begin{cases} \n",
    "1, & \\text{if } X + Y > \\frac{2}{3} \\\\\n",
    "0, & \\text{otherwise}\n",
    "\\end{cases}$\n",
    "\n",
    "$ Y = f(X) = \\begin{cases} \n",
    "1 & \\text{if } x < \\frac{1}{3} \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}$\n",
    "\n",
    "We also use the following ridge functions:\n",
    "\n",
    "1. $ g_{jmp}(w) = 1{\\{w \\geq 1\\}} $\n",
    "2. $ g_{knk}(w) = \\frac {\\min(\\max(âˆ’2, w), 1) + 2} {3} $\n",
    "3. $ g_{smo}(w) = \\Phi (w)$\n",
    "4. $ g_{fin}(w) = \\min(1,\\sqrt{\\max(w + 2, 0)}/2) $\n",
    "\n",
    "$w = \\frac{1}{\\sqrt{d}} \\sum_{j=1}^{d}\\Phi^{-1}(x_{j})$, $\\Phi(.)$ is the CDF of standard Normal Distribution on R, denoted by $ \\mathcal{N}(0,1)$, and $x \\sim U(0, 1)^d$.\n",
    "\n",
    "We have used DigitalNetB2 (Sobol) for QMC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the necessary modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import norm,t\n",
    "from scipy.stats import beta,uniform\n",
    "from confseq.betting import betting_ci_seq\n",
    "from confseq.predmix import predmix_empbern_ci_seq\n",
    "import qmcpy as qp\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters used for our numerical experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05 # Significance level, confidence level = 1 - alpha\n",
    "\n",
    "# parameteres used for the beta distribution simulations\n",
    "\n",
    "beta_param = np.array([10,30]) #parameters for the beta distribution\n",
    "\n",
    "# parameters used for the integrand problem\n",
    "\n",
    "# The integrand functions:\n",
    "fs = {\n",
    "    \"smooth_1d\": lambda x: x[...,0]*np.exp(x[...,0])/np.exp(1), \n",
    "    \"discontinuous_1d\": lambda x: (x[...,0]) < (1/3),\n",
    "    \"discontinuous_2d\": lambda x: (x[...,0]+x[...,1])>=(2/3),\n",
    "}\n",
    "# parameters used for the ridge functions\n",
    "\n",
    "# The ridge functions:\n",
    "gs = {\n",
    "    \"jmp\": lambda w: w>=1, \n",
    "    \"knk\": lambda w: ((np.minimum(np.maximum(-2,w),1)) + 2) / 3,\n",
    "    \"smo\": lambda w: norm.cdf(w),\n",
    "    \"fin\": lambda w: np.minimum(1,((np.sqrt(np.maximum(w+2,0)))/2)),\n",
    "}\n",
    "d = np.array([1,2,4,16]) # The different d's to test on\n",
    "ci_methods = np.array([\"CLT\", \"EB\", \"Betting\"]) # The different CI methods\n",
    "\n",
    "# parameters used for the integrand problems and ridge functions\n",
    "\n",
    "\n",
    "N_vary = np.array([2**8,2**10,2**12, 2**15])# The maximum sample size to be used. Recommended to keep a power of 2 since n must be a power of 2 (QMC rules).\n",
    "n_vary = 2 ** np.arange(0, 7) # The vector of number of low discrepancy or QMC samples generated per replication\n",
    "M = 20 # The number of times the computation is repeated\n",
    "\n",
    "# seed settings\n",
    "\n",
    "global_seed = 7\n",
    "parent_seed = np.random.SeedSequence(global_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function to generate IID replications of QMC samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_qmc_samples_iid(discrete_distrib, true_measure, n = 2**8, function = None, ridge = False):\n",
    "    assert isinstance(discrete_distrib,(qp.DigitalNetB2,qp.Lattice,qp.IIDStdUniform))\n",
    "    assert true_measure in [\"uniform\",\"beta\"]\n",
    "    x_rld = discrete_distrib.gen_samples(n).reshape((discrete_distrib.replications,n,discrete_distrib.d))\n",
    "    if true_measure==\"beta\":\n",
    "        x_rld = beta(a=beta_param[0], b=beta_param[1]).ppf(x_rld)\n",
    "    if ridge is True:\n",
    "        return x_rld\n",
    "    if function is None:\n",
    "        y_rld = x_rld[...,0]\n",
    "    else:\n",
    "        y_rld = function(x_rld)\n",
    "    return y_rld.mean(1),y_rld.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function to return the sequence of CLT CI Widths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clt_ci_seq (values, times, alpha = 0.05):\n",
    "    assert np.all(times <= len(values)), f\"Invalid values in times: {times[times > len(values)]}\"\n",
    "    ci_arr = np.zeros(len(times))\n",
    "    for time in range (len(times)):\n",
    "        curr_val = values[0:times[time]]\n",
    "        ci_arr[time] = 2 * (t.ppf(1 - alpha / 2,times[time] - 1) * curr_val.std(ddof=1) / np.sqrt(times[time])) \n",
    "    return ci_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating the QMC samples that will be used in both the ridge and three test functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_qmc_arr = np.empty(len(n_vary), dtype=object)\n",
    "for i in range(len(n_vary)):\n",
    "    R_vary = N_vary.max() // n_vary[i]\n",
    "    child_seed = parent_seed.spawn(1)[0]\n",
    "    x_qmc_arr[i] = gen_qmc_samples_iid(discrete_distrib=qp.DigitalNetB2(d[-1],seed = child_seed,replications=(M*R_vary)), true_measure=\"uniform\",n = n_vary[i],ridge = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Ridge Functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Varying the $R$ and $n$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QMC Numerical Experiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_qmc.shape = (655360, 1, 16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/bet_sim/lib/python3.8/site-packages/numpy/core/fromnumeric.py:57\n",
      "\tRuntimeWarning: overflow encountered in accumulate\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_qmc.shape = (327680, 2, 16)\n",
      "x_qmc.shape = (163840, 4, 16)\n",
      "x_qmc.shape = (81920, 8, 16)\n",
      "x_qmc.shape = (40960, 16, 16)\n",
      "x_qmc.shape = (20480, 32, 16)\n",
      "x_qmc.shape = (10240, 64, 16)\n"
     ]
    }
   ],
   "source": [
    "qmc_arr_ridge = np.empty((len(N_vary),M, len(ci_methods),len(d),len(gs), len(n_vary))) # consists of CIs (CLT, EB, Betting) for QMC, the ridge functions, the different dimensions, different R's and n's, and different N's.\n",
    "for i in range (len(n_vary)):\n",
    "    x_qmc = norm.ppf(x_qmc_arr[i])\n",
    "    R_vary = N_vary // n_vary[i]\n",
    "    print(\"x_qmc.shape = %s\"%str(x_qmc.shape))\n",
    "    for j in range (len(d)):\n",
    "        w_qmc = x_qmc[:, :, :d[j]].sum(axis = 2)/np.sqrt(d[j])\n",
    "        m_counter = 0\n",
    "        for m in range (M):\n",
    "            m_qmc = w_qmc[m_counter:m_counter + R_vary.max()]\n",
    "            counter = 0\n",
    "            for g in gs.values():\n",
    "                y = g(m_qmc).mean(axis = 1)\n",
    "                qmc_arr_ridge[:,m,0,j,counter,i] = clt_ci_seq(y,times = R_vary,alpha=alpha) # CLT CI widths\n",
    "                lower_bound_qmc_integrand_eb,upper_bound_qmc_integrand_eb = predmix_empbern_ci_seq(y, times=R_vary, alpha=alpha, parallel=False, truncation =1/2) \n",
    "                # Getting the sequential EB CI widths according to the code from the paper above\n",
    "                qmc_arr_ridge[:,m,1,j,counter,i] = upper_bound_qmc_integrand_eb[0] - lower_bound_qmc_integrand_eb[0] # The EB CI based on N_vary\n",
    "                lower_bound_qmc_integrand_bet,upper_bound_qmc_integrand_bet = betting_ci_seq(y, times=R_vary, alpha=alpha, parallel=False, m_trunc=True, trunc_scale=3 / 4) \n",
    "                # Getting the sequential Betting CI widths according to the code from the paper above\n",
    "                qmc_arr_ridge[:,m,2,j,counter,i] = upper_bound_qmc_integrand_bet[0] - lower_bound_qmc_integrand_bet[0] # The Betting CI based on N_vary\n",
    "                counter = counter + 1\n",
    "            m_counter = m_counter + R_vary.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we plot how the CLT, EB, and Betting CI width based on a total of N_vary changes as R and n changes for the different dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(len(d),len(gs),figsize=(20, 18))\n",
    "for k in range (len(d)):\n",
    "    for counter, name in enumerate(gs.keys()):\n",
    "        for methods in range (len(ci_methods)):\n",
    "            axs[k,counter].plot(n_vary, qmc_arr[methods,k,counter,:], label=f\"{ci_methods[methods]}\")\n",
    "        axs[k,counter].set_xlabel(\"samples generated per replication (n)\")\n",
    "        axs[k,counter].set_ylabel(\"CI Width\")\n",
    "        axs[k,counter].set_title(f\"{name}($d = {d[k]}$)\")\n",
    "        axs[k,counter].legend()\n",
    "        axs[k,counter].set_xscale('log', base = 2)\n",
    "        axs[k,counter].set_yscale('log')\n",
    "fig.text(0.3,1,\"CI Width vs n for different ridge functions, dimensions, and CI Methods\", fontsize = 14)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "* In general, CLT < Betting < EB for the CI widths, which is to be expected.\n",
    "* The optimum n is about $2^{10}$ or close to that for CLT. Smaller for Betting and EB at $n = 2$ or $n = 4$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we print the $R$ and $n$ size at which we get the minimum CI according to the CLT, EB, and Betting CI method and compare it to the width for IID:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dim in range (len(d)):\n",
    "    print(\"\\nFor d =\", d[dim],\":\")\n",
    "    counter = 0\n",
    "    for name in gs.keys():\n",
    "        print(\"\")\n",
    "        print(\"For ridge function\", name,\":\")\n",
    "        for ci in range(len(ci_methods)):\n",
    "            print(\"The IID\",ci_methods[ci], \"width =\", iid_arr[ci,dim,counter])\n",
    "            min_val = np.min(qmc_arr[ci,dim,counter,:])\n",
    "            min_indices = np.where(qmc_arr[ci,dim,counter,:] == min_val)[0]\n",
    "            print(\"The IID_QMC\",ci_methods[ci], \"width is minimum when R =\",R_vary[min_indices],\"n =\", n_vary[min_indices],\"and the width =\", min_val)\n",
    "        counter = counter + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "* IID_QMC tends to perform better than IID. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Integrand Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Varying $R$ and $n$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we vary the R and n for IID replication of QMC samples while keeping their product a constant. Note that R x n = N_vary. We will then identify the case where we get the minimum width for Betting CI and empirical Bernstein CI and compare it to IID. We will also compare the two CI methods:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IID replications of QMC experiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_qmc.shape = (655360, 1, 16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/bet_sim/lib/python3.8/site-packages/numpy/core/fromnumeric.py:57\n",
      "\tRuntimeWarning: overflow encountered in accumulate\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_qmc.shape = (327680, 2, 16)\n",
      "x_qmc.shape = (163840, 4, 16)\n",
      "x_qmc.shape = (81920, 8, 16)\n",
      "x_qmc.shape = (40960, 16, 16)\n",
      "x_qmc.shape = (20480, 32, 16)\n",
      "x_qmc.shape = (10240, 64, 16)\n"
     ]
    }
   ],
   "source": [
    "qmc_arr_func = np.empty((len(N_vary),M, len(ci_methods),len(fs), len(n_vary))) # consists of CIs (CLT, EB, Betting) for QMC, the integrands, different R's and n's, and different N's.\n",
    "for i in range (len(n_vary)):\n",
    "    R_vary = N_vary // n_vary[i]\n",
    "    x_qmc = x_qmc_arr[i]\n",
    "    print(\"x_qmc.shape = %s\"%str(x_qmc.shape))\n",
    "    m_counter = 0\n",
    "    for m in range (M):\n",
    "        m_qmc = x_qmc[m_counter:m_counter + R_vary.max()]\n",
    "        counter = 0\n",
    "        for f in fs.values():\n",
    "            y = f(m_qmc).mean(axis = 1)\n",
    "            qmc_arr_func[:,m,0,counter,i] = clt_ci_seq(y,times = R_vary,alpha=alpha) # CLT CI widths\n",
    "            lower_bound_qmc_integrand_eb,upper_bound_qmc_integrand_eb = predmix_empbern_ci_seq(y, times=R_vary, alpha=alpha, parallel=False, truncation =1/2) \n",
    "            # Getting the sequential EB CI widths according to the code from the paper above\n",
    "            qmc_arr_func[:,m,1,counter,i] = upper_bound_qmc_integrand_eb[0] - lower_bound_qmc_integrand_eb[0] # The EB CI based on N_vary\n",
    "            lower_bound_qmc_integrand_bet,upper_bound_qmc_integrand_bet = betting_ci_seq(y, times=R_vary, alpha=alpha, parallel=False, m_trunc=True, trunc_scale=3 / 4) \n",
    "            # Getting the sequential Betting CI widths according to the code from the paper above\n",
    "            qmc_arr_func[:,m,2,counter,i] = upper_bound_qmc_integrand_bet[0] - lower_bound_qmc_integrand_bet[0] # The Betting CI based on N_vary\n",
    "            counter = counter + 1\n",
    "        m_counter = m_counter + R_vary.max()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we plot how the Betting CI and EB CI width based on a total of N_vary changes as R and n change:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(n_vary,ci_vector_qmc_integrand_bet, label = \"Betting CI\");\n",
    "plt.plot(n_vary,ci_vector_qmc_integrand_eb, label = \"EB CI\");\n",
    "plt.xlabel(\"samples generated per replication (n)\");\n",
    "plt.ylabel(\"CI width\");\n",
    "plt.title(\"CI width vs n of QMC for $f(X)$ integrand\");\n",
    "plt.legend();\n",
    "plt.xscale('log', base = 2)\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some further observations:\n",
    "* The CI width for both methods initially tends to decrease as number of IID replication (R) increases but tends to increase later\n",
    "* Betting performs better for the most part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we print the R and n size at which we get the minimum CI according to the Betting and EB CI method and compare it to the width for IID:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_ci_qmc_bet = np.min(ci_vector_qmc_integrand_bet) # The smallest Betting CI width for IID replications of QMC\n",
    "min_ci_index_qmc_bet = np.argmin(ci_vector_qmc_integrand_bet) # The index at which we get the smallest Betting CI width for IID Replications of QMC\n",
    "min_ci_qmc_eb = np.min(ci_vector_qmc_integrand_eb) # The smallest EB CI width for IID replications of QMC\n",
    "min_ci_index_qmc_eb = np.argmin(ci_vector_qmc_integrand_eb) # The index at which we get the smallest EB CI width for IID Replications of QMC\n",
    "\n",
    "print(\"IID Betting CI width at sample size N_vary =\",N_vary, \"is\", ci_iid_integrand_bet)\n",
    "print(\"IID EB CI width at sample size N_vary =\",N_vary, \"is\", ci_iid_integrand_eb)\n",
    "print(\"\")\n",
    "print(\"IID Replications of QMC Betting CI width based on sample size N_vary =\",N_vary, \"is lowest when R =\",R_vary[min_ci_index_qmc_bet],\n",
    "      \"and n =\",n_vary[min_ci_index_qmc_bet],\"\\nThe CI width for this R and n is\", min_ci_qmc_bet)\n",
    "print(\"\")\n",
    "print(\"IID Replications of QMC EB CI width based on sample size N_vary =\",N_vary, \"is lowest when R =\",R_vary[min_ci_index_qmc_eb],\n",
    "      \"and n =\",n_vary[min_ci_index_qmc_eb],\"\\nThe CI width for this R and n is\", min_ci_qmc_eb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some further observations:\n",
    "* The CIs through IID replications of QMC perform better than IID\n",
    "* Betting CI performs better than EB CI for QMC and similar for plain IID (a bit better for EB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some further observations:\n",
    "* The CI width for both methods initially tends to decrease as number of IID replication (R) increases but tends to slightly increase later\n",
    "* Betting CI performs better for most part"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bet_sim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
